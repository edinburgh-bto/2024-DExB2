{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with K nearest neighbors (kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. kNN Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Type of algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we learned that kNN is a supervised machine learning methods. It can be used for both classification and regression problems. This means that we are given a labelled dataset consiting of training observations $(x,y)$ and would like to capture the relationship between the features $x$ and the outcomes $y$. Here, we will use kNN for classification. This means that $y$ takes categorical values, and the task then is to predict the category $y$ for new observations of $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Distance measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the classification setting, the k-nearest neighbor algorithm essentially boils down to forming a majority vote between the k most similar instances to a given “unseen” observation. Similarity is defined according to a distance metric between two data points. The k-nearest-neighbor classifier is commonly based on the Euclidean distance between a test sample and the specified training samples. Let $x^{(i)}$ be data point with $p$ features $(x^{(i)}_{1}, x^{(i)}_{2},..., x^{(i)}_{p})$, $N$ be the total number of data points $(i=1,2,...,N)$.  The Euclidean distance between $x^{(i)}$ and $x^{(j)}$ is defined as: \n",
    "\n",
    "\n",
    "$$d(x^{(i)}, x^{(j)}) = \\sqrt{(x^{(i)}_{1} - x^{(j)}_{1})^2 + (x^{(i)}_{2} - x^{(j)}_{2})^2 + ... + (x^{(i)}_{p} - x^{(j)}_{p})^2}$$\n",
    "\n",
    "Sometimes other measures can be more suitable for a given setting and include the Manhattan, Chebyshev and Hamming distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Algorithm steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kNN algorithm consists of four simple steps:\n",
    "\n",
    "    (1) Choose the number k of neighbors.\n",
    "\n",
    "    (2) Take the k nearest neighbors of the new data point, according to your distance metric.\n",
    "\n",
    "    (3) Among these k neighbors, count the number of data points to each category.\n",
    "\n",
    "    (4) Assign the new data point to the category with the highest count among neighbours of the new data point.\n",
    "    \n",
    "Throughout the notebook we will use a number of libraries and functions. Check out their manuals to familiarize yourselves with the methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing and preperation of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load some libraries we will use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyse the Iris data set you have already familiarised yourself with earlier during the course. It includes 3 iris species with 50 samples each. For each sample, we have 4 features: sepal length and width as well as petal length and width, all in cm. Here, species is a categorical outcome that we aim to predict from the numeric features using KNN.\n",
    "\n",
    "__TASK:__ Use ```pd.read_csv()``` to load the Iris data set in ```Iris.csv```, which located in the same folder as your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Summarize the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start, it is always best to get a quick glimpse of the data and its format. We can get a quick idea of how many samples (rows) and how many attributes (columns) the data contains using ```shape()``` on the ```dataset``` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using shape to get a first idea of the data format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Using ```head(n)```  on the ```dataset``` object we can see the first $n$ lines of the data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the first 5 rows of the data table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using describe(), we can get some summary statistics of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show summary statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now take a look at the number of instances (rows) that belong to each class. We can view this as an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.groupby('Species').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Dividing data into features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see dataset contain six columns: Id, SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm and Species. The actual features are described by columns 1-4. The last column contains labels of samples. Firstly we need to split data into two arrays: X (features) and y (labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm','PetalWidthCm']\n",
    "X = dataset[feature_columns].values\n",
    "y = dataset['Species'].values\n",
    "\n",
    "# Alternative way of selecting features and labels arrays:\n",
    "# X = dataset.iloc[:, 1:5].values\n",
    "# y = dataset.iloc[:, 5].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our species labels are categorical, encoded in character strings. The KNN function, KNeighborsClassifier, which we will use, does not accept string labels. We therefore use LabelEncoder to transform them into numbers: Iris-setosa correspond to 0, Iris-versicolor correspond to 1 and Iris-virginica correspond to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Spliting dataset into training set and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, it is common to split datasets into training and test data sets. We will learn more about this during the next lecture. \n",
    "\n",
    "Essentially, splitting the data we can test the model performance on data that is independent from the training data. The training data is used to 'train' our machine learning model, i.e. to adjust the method's parameters, and the test dataset is then used to assess the performance of the model on previously unseen data using an appropriate metric of performance. \n",
    "\n",
    "Let's split dataset into training set and test set, to check later on whether or not our classifier works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Often, different features can take values on very different scales, in which case it would make sense to scale or 'normalise' data before further analysis. In our case, the different features, sepal and petal lengths and widths, take similar values, and we therefore don't have to worry about normalisation. Just keep in mind that in other data scenarios it can be extremly important to apply feature scaling before running classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visual data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In __§2.3__, we took an initial look at the data format. Equally important is looking at our data by means of different visualisations to get a feel of the data before starting any further model analysis via machine learning. In the following, we will plot the data in different ways. \n",
    "\n",
    "__TASK__: Think about which plots you find most helpful and why!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Parallel Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel coordinates is a plotting technique for plotting multivariate data. It allows one to see clusters in data and to estimate other statistics visually. Using parallel coordinates, single data points are represented as connected line segments with features along the x-axis and their values on the y-axis. Each connected line segment represents one data point, and we can colour the lines by the outcome label (species). Points that tend to cluster will appear closer together.\n",
    "\n",
    "__TASK__: Using the ```parallel_coordinates()``` function from the ```pandas.plotting``` library, make a parallel coordinates plot with lines coloured by 'Species'. Tip: Make sure to remove the Id column using ```dataset.drop(\"Id\", axis=1)``` instead of the full dataset. Make sure to label the axes and provide a legend for the species colours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "# use parallel_coordinates()\n",
    "\n",
    "# add a title, axes labels and a legend\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Pairplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multivariate analysis we are concerned with the relations between various data variables. Feature variables may be statistically independent or display statistical dependence. To get an idea, it often helps to look at these dependencies beforehand, but visualisation in high dimensions are tricky, and 2- or at most 3-dimensional visualisations are more intuitive to us. \n",
    "\n",
    "Looking at pairwise dependencies is useful when you want to visualize the relationship between multiple variables separately within subsets of your dataset.\n",
    "\n",
    "__TASK:__ Use the ```pairplot()``` function from seaborn to make pairwise scatterplots, grouped by 'Species', along with the 1-d marginal distributions of features. Again, make sure to drop the Id-column in dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# use pairplot()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# NOTE, there will be a UserWarning due to a bug in matplotlib - ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplots are a simple way to visualise 1-d distributions. \n",
    "\n",
    "__TASK:__ Make boxplots of the 4 features grouped by species. Tip: The function ```boxplot()``` is applied directly to your dataset object (with the Id column dropped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# use boxplot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.4. Alternatives to Boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Box plots are not everyone's favourite, in fact, some journals have banned them. We can use __strip plots__ or __violine plots__ as an alternative. Why do you think people would prefer those over box plots?\n",
    "Which of the three do you prefer and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.4.1. Strip plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TASK:__ Make a figure with four subplots, for each feature, and use the ```stripplot``` function from seaborn, grouping by species. Using the ```size``` property you can control the size of dots in the plots, which can be helpful to better see the data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# strip plots of the data\n",
    "plt.figure(figsize=(10,10))\n",
    "# creating four subplots, one for each feature, arranged in 2 rows and 2 columns\n",
    "plt.subplot(2,2,1)\n",
    "# use stripplot in each of the subplots\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1. Violin plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TASK:__ Make a figure with four subplots, for each feature, and use the ```violinplot``` function from seaborn, grouping by species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# violin plots\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,2,1)\n",
    "# same as above for stripplot, now with violinplot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.5. Other options for visualisation of high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is also useful to look at more than 2 variables together. We can use 3D plots to look at 3 variables together at a time, using color, shape, size and other properties of 3D and 2D objects. \n",
    "\n",
    "__Optional task:__ Have a go and use ```scatter()``` to make a 3D scatter plot using marker sizes to visualize the fourth dimenssion which is Petal Width [cm]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(1, figsize=(20, 15))\n",
    "ax = Axes3D(fig)\n",
    "# create scatterplot, plotting X[:,i], for i = 0,1,2 and using the 4th feature X[:,3] as marker size\n",
    "\n",
    "# adding class labels to the mean points in each class\n",
    "for name, label in [('Virginica', 0), ('Setosa', 1), ('Versicolour', 2)]:\n",
    "    ax.text3D(X[y == label, 0].mean(),\n",
    "              X[y == label, 1].mean(),\n",
    "              X[y == label, 2].mean(), name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'),size=25)\n",
    "\n",
    "# add title and axes labels\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Using KNN for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have familiarised ourselves with the data, let's use the KNN algorithm to predict species lables from the sepal and petal features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TASK:__ Use ```KNeighborsClassifier()``` from ```sklearn.neighbors``` to first construct a KNN classifier object and then use ```fit()``` on the object to build the classifier from the training set ```X_train```. Finally use ```predict()``` on the classifier object to make predictions for the test dataset ```X_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting clasifier to the Training set\n",
    "# Loading libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate model (k = 3)\n",
    "\n",
    "\n",
    "# Fitting the model on X_train\n",
    "\n",
    "\n",
    "# Predicting the Test set results and store in a list y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Evaluating predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building confusion matrix using the function ```confusion_matrix()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and print out the confusion matrix\n",
    "cm = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating model accuracy using ```accuracy_score()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and print the accuracy of the classifier\n",
    "accuracy = \n",
    "print('\\n Accuracy of our model is ' + str(round(accuracy*100, 2)) + '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TASK:__ Reflect on the above result! Are you happy with this accuracy? Is accuracy indeed the best metric to use here, and if so, why? Repeat __§4.1__ & __§4.3__ with a different $k$ - when do you get better or worse predictions? Do you think the estimated accuracy is reliable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Using cross-validation for parameter tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a technique to systematically split up the training and test sets in different ways so as to obtain more robust performance estimates. We will learn more about cross-validation in the next lecture. For now, let's try to use it to determine the best value of $k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TASK:__ For a list of values $k\\in[1, ..., 50]$ build a KNN classifier using again ```KNeighborsClassifier``` and then use the function ```cross_val_score()``` on ```X_train``` and ```y_train``` to obtain robust estimates of the accuracy of the classifiers with different $k$. Store the results in a list ```cv_scores```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of K for KNN\n",
    "k_list = list(range(1,50,2))\n",
    "# creating list of cross validation scores\n",
    "cv_scores = []\n",
    "\n",
    "# for each value of k, perform 10-fold cross validation\n",
    "for k in k_list:\n",
    "    # instantiate the KNN classifier object with the current value of k\n",
    "\n",
    "    # compute the accuracy scores for each iteration of the cross-validation using cross_val_score()\n",
    "    scores = \n",
    "    # append the mean of scores to cv_scores using the append() function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TASK:__ Let's plot the misclassification error, $MSE = 1 - $accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the misclassification error from cv_scores\n",
    "MSE = \n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(15,10))\n",
    "# plot the list of k values (k_list) against the corresponding misclassification errors\n",
    "\n",
    "# add a title and axes labels\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__REFLECT:__ The best $k$-NN classifier is the one that minimises the misclassification error. Find out what the $k$ is. Now look at the trend of the misclassification error - would you have expected that the error goes up again for higher $k$, and if so why? Google the term 'overfitting'. How can you detect overfitting from the above curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding best k\n",
    "best_k = k_list[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbors is %d.\" % best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TASK:__ Above we have applied 10-fold cross-validation. Repeat the cross-validation analysis at differen fold-values, changing the ```cv``` property in the ```cross_val_score()``` function. What changes do you see? By searching 'cross-validation' what insights can you get on how to choose the fold-value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. My own KNN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For the curious-minded:__ The KNN algorithm is relatively simple. Do you think you can implement your own KNN algorithm?\n",
    "\n",
    "__Optional task:__ Start by building defining ```class MyKNeighborsClassifier()```, it should include functions ```__init__(self, params)```, ```fit(self, X, y)``` and ```predict(self, X_test)```. The ```predict()``` function will loop over all new data points and make single predictions for each point. For that, implement a separate function ```single_prediction(X, y, x_train, k)```, which computes the distance of all training data points to the new point and considers the class label of the $k$ nearest neighbours for a vote on the predicted class label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "class MyKNeighborsClassifier():\n",
    "    \"\"\"\n",
    "    My implementation of KNN algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_neighbors=5):\n",
    "        # initialisation only needs to set a property n_neighbors on self\n",
    "\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # X are the features, y the outcomes\n",
    "        \n",
    "        \"\"\"\n",
    "        Fit the model using X as array of features and y as array of labels.\n",
    "        \"\"\"\n",
    "        n_samples = # set to number of samples\n",
    "        \n",
    "        \"\"\"\n",
    "        Some error checks\n",
    "        \"\"\"\n",
    "        # number of neighbors can't be larger then number of samples\n",
    "        if self.n_neighbors > n_samples:\n",
    "            raise ValueError(\"Number of neighbors can't be larger then number of samples in training set.\")\n",
    "        \n",
    "        # X and y need to have the same number of samples\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"Number of samples in X and y need to be equal.\")\n",
    "        \n",
    "        \"\"\"\n",
    "        Find and save all possible class labels using np.unique()\n",
    "        \"\"\"\n",
    "        \n",
    "        self.classes_ = # unique class labels\n",
    "        \n",
    "        # define the X features and y outcomes\n",
    "        self.X = # features\n",
    "        self.y = # outcomes\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        # number of predictions to make and number of features inside single sample using shape on X_test\n",
    "        n_predictions, n_features = X_test.shape\n",
    "        \n",
    "        # allocationg space for array of predictions\n",
    "        predictions = np.empty(n_predictions, dtype=int)\n",
    "        \n",
    "        # loop over all observations\n",
    "        for i in range(n_predictions):\n",
    "            # calculation of single prediction\n",
    "            predictions[i] = single_prediction(self.X, self.y, X_test[i, :], self.n_neighbors)\n",
    "\n",
    "        return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_prediction(X, y, x_test, k):\n",
    "    \n",
    "    # number of samples inside training set\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # create array for distances and targets\n",
    "    distances = np.empty(n_samples, dtype=np.float64)\n",
    "\n",
    "    # distance calculation\n",
    "    for i in range(n_samples):\n",
    "        distances[i] = # calculate the Euclidian distance between x_test and X using the formula in §1.2\n",
    "    \n",
    "    # combining arrays as columns\n",
    "    distances = sp.c_[distances, y]\n",
    "    # sorting array by value of first column\n",
    "    sorted_distances = distances[distances[:,0].argsort()]\n",
    "    # selecting labels associeted with k smallest distances\n",
    "    targets = sorted_distances[0:k,1]\n",
    "\n",
    "    unique, counts = np.unique(targets, return_counts=True)\n",
    "    return(unique[np.argmax(counts)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instantiate and fit your KNN model on the training set and make predictions for ```X_test``` for a $k$ of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate learning model (k = 3)\n",
    "my_classifier = # instantiate MyKNeighborsClassifier()\n",
    "\n",
    "# Fitting the model with X_train and y_train\n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "my_y_pred = # make predictions on X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TASK:__ Compute the accuracy of your KNN classifier and compare to the result above using ```KNeighborsClassifier()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and print the accuracy of the model\n",
    "accuracy = \n",
    "print('Accuracy of our model is equal ' + str(round(accuracy*100, 2)) + ' %.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

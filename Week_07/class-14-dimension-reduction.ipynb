{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction Methods\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "Today we learned about dimensionality reduction and principal component analysis in particular. Here in this notebook, we will use PCA to revisit the breast cancer data from our last session. This will save us some time re exploratory data analysis, so we will have more time to focus and think about how dimension reduction can help us in our analytic workflow. \n",
    "\n",
    "If necessary, refresh your memory on the dataset here: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import and data prep\n",
    "### 2.1. Importing Dependencies\n",
    "\n",
    "As usual, we start by importing libraries we will use later on. Throughout the notebook, if any functions are unclear, try googling the library and function to familiarize yourself with the functions and their in- and outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn import decomposition\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Load data\n",
    "\n",
    "**TASK:** Load the file ```breast-cancer-wisconsin.csv``` located in the same directory as this notebook using ```read_csv()``` from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Data summary\n",
    "\n",
    "As usual, we start by having a peak at the data. \n",
    "\n",
    "**TASK:** Use ```head()```, ```describe()``` and ```info()``` on the dataframe to get a first idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Data preparation\n",
    "\n",
    "#### 2.4.1. Data clean-up\n",
    "\n",
    "**TASK:** We don't need the first and last column ('id', 'Unnamed: 32') - drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and modifying the data\n",
    "data = data.drop(...)\n",
    "data = data.drop(...)\n",
    "\n",
    "ndat, nvar = data.shape\n",
    "ndat, nvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2. Label encoding\n",
    "\n",
    "Finally, we map the diagnostic values to binary values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['diagnosis'] = data['diagnosis'].map({'M':1,'B':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Assign the data to ```X```, excluding ```diagnosis```, and assign the latter to ```y```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, ...]\n",
    "\n",
    "y = data.loc[:, 'diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3. Data scaling\n",
    "\n",
    "We learned about the importance of scaling data whenever working with distance-based methods.\n",
    "\n",
    "**TASK:** Use the function ```StandardScaler()``` to scale the data. Use ```describe()``` to see how the data has changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = ...\n",
    "scaled_X = ...\n",
    "pd.DataFrame(scaled_X, columns=X.columns).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Compare the scaled data to the original unscaled data - what did the scaling do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory data analysis\n",
    "\n",
    "**TASK:** Check out last session's notebook on clustering to refresh your memory on the dataset. Perhaps you can think of aspects to explore that you didn't check last time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. For the mathematically curious\n",
    "\n",
    "Using the ```linalg``` library from numpy, compute the eigenvalues of the above covariance matrix. Create two bar plots of the unnormalized and normalized eigenvalues (i.e. divided by the sum of eigenvalues). We can check later if they coincide with the explained variances by the PC's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "\n",
    "# compute covariance matrix of transposed of scaled_X\n",
    "cov_matrix = ...\n",
    "\n",
    "# compute eigenvalues of the covariance matrix using linalg.eig()\n",
    "eigenvalues, eigenvectors = ...\n",
    "\n",
    "df1 = pd.DataFrame({'Eigenvalues Covariance matrix': eigenvalues,\n",
    "                    'Eigenvalue number': [str(x) for x in range(1,nvar)]})\n",
    "\n",
    "plt.figure(figsize = (25,6))\n",
    "sns.barplot(x = 'Eigenvalue number',y = 'Eigenvalues Covariance matrix',data = df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "\n",
    "# plot as above but divide the eigenvalues by their sum\n",
    "df1 = pd.DataFrame({'Normalized eigenvalues': ...,\n",
    "                    'Eigenvalue number': [str(x) for x in range(1,nvar)]})\n",
    "\n",
    "plt.figure(figsize = (25,6))\n",
    "sns.barplot(x = 'Eigenvalue number',y = 'Normalized eigenvalues',data = df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Principal Component Analysis (PCA)\n",
    "Now we will use PCA to find a linear projection of the variable space. First, we will look at the full PCA space to then decide how to reduce it. \n",
    "\n",
    "### 4.1. Full PCA\n",
    "\n",
    "**TASK:** Create a PCA object called ```pca``` using ```decomposition.PCA()```, then transform the data into the PCA space by using ```fit_transform``` on ```scaled_X```. Check out the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = ...\n",
    "tdata = ...\n",
    "tdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflect:** Have the dimensions been reduced? What is the new transformed data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Analysis of the PCA space\n",
    "\n",
    "**TASK:** Check out the ```explained_variance_``` and ```explained_variance_ratio_``` attributes of the pca object. Create bar plots of them and compare to the bar plots of the eigenvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Make a scree plot based on the ```explained_variance_ratio_```. Is there a clear scree point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'% variation explained': ...,\n",
    "                    'PCs':['PC' + str(x) for x in range(1,nvar)]})\n",
    "\n",
    "plt.figure(figsize = (25,6))\n",
    "sns.barplot(x = 'PCs',y = '% variation explained',data = df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Now plot the cumulative explained variance ratios using ```np.comsum()```. Do you find this or the scree plot more informative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'% variation explained': ...,\n",
    "                    'Number of PCs': [str(x) for x in range(1,nvar)]})\n",
    "\n",
    "plt.figure(figsize = (25,6))\n",
    "sns.barplot(x = 'Number of PCs',y = '% variation explained',data = df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflect:** Look at the scree and the cumulative scree plots. How many components are required to explain at least 80% of the variance in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional TASK:** If you checked out the eigenvalues of the covariance matrix in the optional task in **ยง3.1**, now check out the ```explained_variance_``` attribute of our pca object. Make a plot and compare both this and the scree plot to those in **ยง3.1**. How do they relate to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'Variation explained':...,\n",
    "                    'PCs':['PC' + str(x) for x in range(1,nvar)]})\n",
    "\n",
    "plt.figure(figsize = (25,6))\n",
    "sns.barplot(x = 'PCs',y = 'Variation explained',data = df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Reduced PCA\n",
    "\n",
    "\n",
    "Above, we learned that 5 components are required to explain at least 80% of the variance in our data. Let's now do dimension reduction by only looking at the first 5 components. \n",
    "\n",
    "**TASK:** Again using ```decomposition.PCA()``` now compute a PCA projection of the data setting ```n_components=5```. Then transform ```scaled_X``` into the reduced PCA space and store in ```pca_5var```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will do PCA with only 5 components now as they seem to provide 80% of the information.\n",
    "pca_red = ...\n",
    "pca_5var = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Check out ```explained_variance_``` and its sum to make sure we really explained at least 80% of our data variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHECK:** We should see that over 80% information is obtained in first 5 components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Create a dataframe ```new_X``` from the transformed data using ```pd.DataFrame()``` on the transformed data, i.e. using ```columns=['PC1','PC2','PC3','PC4','PC5']``` of ```pca_5var```. Check it out using ```head()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** In the lecture, we learned that PCA makes a rotated projection of the data, where correlations have been removed. check if that's true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.heatmap(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see that there is no correlation among the principal components which is very important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification of the breast cancer data\n",
    "\n",
    "### 5.1. On the reduced PCA space\n",
    "\n",
    "We will now train LogisticRegression on the data projected onto the 5 principal components.\n",
    "\n",
    "**Reflect:** Why would we choose logistic regression on this data? \n",
    "\n",
    "Logistic regression is a classification method for binary outcome data.\n",
    "\n",
    "**TASK:** Use ```train_test_split()``` on ```new_X``` and ```y``` to create a test-train data split with ```test_size=0.2``` and setting ```random_state``` to any number of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train, new_X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Create a model called ```logreg``` using ```LogisticRegression()``` and setting ```solver='lbfgs'```. Fit the model on the training data using ```fit()``` and then make predictions on the test set using ```predict()``` on the logreg model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logreg model\n",
    "logreg = ...\n",
    "\n",
    "# fit on training data\n",
    "\n",
    "\n",
    "# predict on test data\n",
    "y_pred_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Print the confusion matrix and the accuracy of the logreg model using ```confusion_matrix()``` and ```accuracy_score()``` on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. On the original data space\n",
    "\n",
    "Now let's see how the regression on the reduced PCA space compares to regression on the original data space. \n",
    "\n",
    "**TASK:** Use ```train_test_split()``` on ```X``` and ```y``` to create a test-train data split with ```test_size=0.2``` and setting ```random_state``` to any number of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's compare Logistic Regession without PCA when we have all of the original features\n",
    "X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Create a model called ```logreg2``` using ```LogisticRegression()``` and setting ```solver='lbfgs'```. Fit the model on the training data using ```fit()``` and then make predictions on the test set using ```predict()``` on the ```logreg2``` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg2 = ...\n",
    "\n",
    "# fit on training data\n",
    "\n",
    "# predict on test data\n",
    "y_pred_test2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Print the confusion matrix and the accuracy of the logreg2 model using ```confusion_matrix()``` and ```accuracy_score()``` on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Compare the accuracy we got for classification when using the original data and when it's been projected onto the reduced PCA space. \n",
    "\n",
    "**Optional TASK:** Can we get even better classification results by using more principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Visualizing the classification results\n",
    "\n",
    "Let's visualize the classification results on the whole data set.\n",
    "\n",
    "**TASK:** First make predictions from the logreg models fitted on the reduced PCA and the original space, i.e. ```logreg``` and ```logreg2```, this time on the whole dataset, i.e. on ```new_X``` and ```X```, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_all = ...\n",
    "y_pred_all2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Make scatter plots of the data projected onto the first two components, i.e. plotting ```new_X.PC1``` vs ```new_X.PC2```, and colour (```c=...```) the points by \n",
    "1. Predictions from classification on the reduced PCA space.\n",
    "2. Predictions from classification on the original data space.\n",
    "3. True diagnoses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True)\n",
    "\n",
    "ax1.scatter(..., cmap = \"jet\", edgecolor = \"None\", alpha=0.35)\n",
    " \n",
    "ax2.scatter(..., cmap = \"jet\", edgecolor = \"None\", alpha=0.35)\n",
    "\n",
    "ax3.scatter(..., cmap = \"jet\", edgecolor = \"None\", alpha=0.35)\n",
    "\n",
    "# don't forget to annotate: title and axis labels\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Since accuracy is quite high for both classifiers, both predictors are very close to the true diagnoses. We know however that we main some accuracy gains, and we can keep in mind next time we do classification and achieve low accuracy that PCA or dimension reduction, more generally, can help us improve predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook has been adapted from https://www.kaggle.com/code/jyotiprasadpal/dimension-reduction-methods/notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
